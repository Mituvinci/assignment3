{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2NLayer.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[],"collapsed_sections":[]}},"cells":[{"metadata":{"id":"A3Aunwngmwad","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["from __future__ import print_function\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from past.builtins import xrange\n","import math, time\n","\n","class TwoLayerNet(object):\n","  \"\"\"\n","  A two-layer fully-connected neural network. The net has an input dimension of\n","  N, a hidden layer dimension of H, and performs classification over C classes.\n","  We train the network with a softmax loss function and L2 regularization on the\n","  weight matrices. The network uses a ReLU nonlinearity after the first fully\n","  connected layer.\n","\n","  In other words, the network has the following architecture:\n","\n","  input - fully connected layer - ReLU - fully connected layer - softmax\n","\n","  The outputs of the second fully-connected layer are the scores for each class.\n","  \"\"\"\n","\n","  def __init__(self, input_size, hidden_size, output_size, std=1e-4):\n","    \"\"\"\n","    Initialize the model. Weights are initialized to small random values and\n","    biases are initialized to zero. Weights and biases are stored in the\n","    variable self.params, which is a dictionary with the following keys:\n","\n","    W1: First layer weights; has shape (D, H)\n","    b1: First layer biases; has shape (H,)\n","    W2: Second layer weights; has shape (H, C)\n","    b2: Second layer biases; has shape (C,)\n","\n","    Inputs:\n","    - input_size: The dimension D of the input data.\n","    - hidden_size: The number of neurons H in the hidden layer.\n","    - output_size: The number of classes C.\n","    \"\"\"\n","    self.params = {}\n","    self.params['W1'] = std * np.random.randn(input_size, hidden_size)\n","    self.params['b1'] = np.zeros(hidden_size)\n","    self.params['W2'] = std * np.random.randn(hidden_size, output_size)\n","    self.params['b2'] = np.zeros(output_size)\n","\n","  def loss(self, X, y=None, reg=0.0):\n","    \"\"\"\n","    Compute the loss and gradients for a two layer fully connected neural\n","    network.\n","\n","    Inputs:\n","    - X: Input data of shape (N, D). Each X[i] is a training sample.\n","    - y: Vector of training labels. y[i] is the label for X[i], and each y[i] is\n","      an integer in the range 0 <= y[i] < C. This parameter is optional; if it\n","      is not passed then we only return scores, and if it is passed then we\n","      instead return the loss and gradients.\n","    - reg: Regularization strength.\n","\n","    Returns:\n","    If y is None, return a matrix scores of shape (N, C) where scores[i, c] is\n","    the score for class c on input X[i].\n","\n","    If y is not None, instead return a tuple of:\n","    - loss: Loss (data loss and regularization loss) for this batch of training\n","      samples.\n","    - grads: Dictionary mapping parameter names to gradients of those parameters\n","      with respect to the loss function; has the same keys as self.params.\n","    \"\"\"\n","    # Unpack variables from the params dictionary\n","    W1, b1 = self.params['W1'], self.params['b1']\n","    W2, b2 = self.params['W2'], self.params['b2']\n","    N, D = X.shape\n","\n","    # Compute the forward pass\n","    scores = None\n","    #############################################################################\n","    # TODO: Perform the forward pass, computing the class scores for the input. #\n","    # Store the result in the scores variable, which should be an array of      #\n","    # shape (N, C).                                                             #\n","    #############################################################################\n","    a1 = np.maximum( X.dot(W1) + b1, 0 )\n","    a2 = a1.dot(W2) + b2\n","\n","    scores = a2\n","    #############################################################################\n","    #                              END OF YOUR CODE                             #\n","    #############################################################################\n","    \n","    # If the targets are not given then jump out, we're done\n","    if y is None:\n","      return scores\n","    \n","    Y = np.zeros(a2.shape)\n","    for i,row in enumerate(Y):\n","        row[y[i]] = 1\n","\n","    # Compute the loss\n","    loss = 0.0\n","    #############################################################################\n","    # TODO: Finish the forward pass, and compute the loss. This should include  #\n","    # both the data loss and L2 regularization for W1 and W2. Store the result  #\n","    # in the variable loss, which should be a scalar. Use the Softmax           #\n","    # classifier loss.                                                          #\n","    #############################################################################\n","    exp_a2 = pow(math.e, a2)\n","    scores   = (exp_a2.T/np.sum(exp_a2, axis = 1)).T\n","    \n","    for i in range(N):\n","      loss -= np.log( scores[i][y[i]] )\n","    loss /= N\n","    loss += 0.5*reg*np.sum(W1 * W1)  \n","    loss += 0.5*reg*np.sum(W2 * W2) \n","    #############################################################################\n","    #                              END OF YOUR CODE                             #\n","    #############################################################################\n","\n","    # Backward pass: compute gradients\n","    grads = {}\n","    #############################################################################\n","    # TODO: Compute the backward pass, computing the derivatives of the weights #\n","    # and biases. Store the results in the grads dictionary. For example,       #\n","    # grads['W1'] should store the gradient on W1, and be a matrix of same size #\n","    #############################################################################\n","    da2 = scores - Y\n","    dW2 = ((da2.T).dot(a1)).T/N + reg*W2\n","    db2 = (da2.T).dot(np.ones(a1.shape[0]))/N\n","    da1 = da2.dot(W2.T)\n","    dW1 = (((da1*(a1>0)).T).dot(X)).T/N + reg*W1\n","    db1 = (((da1*(a1>0)).T).dot(np.ones(X.shape[0]))).T/N\n","      \n","    grads['W2'] = dW2\n","    grads['b2'] = db2\n","    grads['W1'] = dW1\n","    grads['b1'] = db1\n","    #############################################################################\n","    #                              END OF YOUR CODE                             #\n","    #############################################################################\n","\n","    return loss, grads\n","\n","  def train(self, X, y, X_val, y_val,\n","            learning_rate=1e-3, learning_rate_decay=0.95,\n","            reg=5e-6, num_iters=100,\n","            batch_size=200, verbose=False):\n","    \"\"\"\n","    Train this neural network using stochastic gradient descent.\n","\n","    Inputs:\n","    - X: A numpy array of shape (N, D) giving training data.\n","    - y: A numpy array f shape (N,) giving training labels; y[i] = c means that\n","      X[i] has label c, where 0 <= c < C.\n","    - X_val: A numpy array of shape (N_val, D) giving validation data.\n","    - y_val: A numpy array of shape (N_val,) giving validation labels.\n","    - learning_rate: Scalar giving learning rate for optimization.\n","    - learning_rate_decay: Scalar giving factor used to decay the learning rate\n","      after each epoch.\n","    - reg: Scalar giving regularization strength.\n","    - num_iters: Number of steps to take when optimizing.\n","    - batch_size: Number of training examples to use per step.\n","    - verbose: boolean; if true print progress during optimization.\n","    \"\"\"\n","    num_train = X.shape[0]\n","    iterations_per_epoch = max(num_train / batch_size, 1)\n","\n","    # Use SGD to optimize the parameters in self.model\n","    loss_history = []\n","    train_acc_history = []\n","    val_acc_history = []\n","\n","    for it in xrange(num_iters):\n","      X_batch = None\n","      y_batch = None\n","\n","      #########################################################################\n","      # TODO: Create a random minibatch of training data and labels, storing  #\n","      # them in X_batch and y_batch respectively.                             #\n","      #########################################################################\n","      batch_indicies = np.random.choice(num_train, batch_size, replace = False)\n","      X_batch = X[batch_indicies]\n","      y_batch = y[batch_indicies]\n","      #########################################################################\n","      #                             END OF YOUR CODE                          #\n","      #########################################################################\n","\n","      # Compute loss and gradients using the current minibatch\n","      loss, grads = self.loss(X_batch, y=y_batch, reg=reg)\n","      loss_history.append(loss)\n","\n","      #########################################################################\n","      # TODO: Use the gradients in the grads dictionary to update the         #\n","      # parameters of the network (stored in the dictionary self.params)      #\n","      # using stochastic gradient descent. You'll need to use the gradients   #\n","      # stored in the grads dictionary defined above.                         #\n","      #########################################################################\n","      for variable in self.params:\n","          self.params[variable] -= learning_rate*grads[variable]\n","\n","      #########################################################################\n","      #                             END OF YOUR CODE                          #\n","      #########################################################################\n","\n","      if verbose and it % 100 == 0:\n","        print('iteration %d / %d: loss %f' % (it, num_iters, loss))\n","\n","      # Every epoch, check train and val accuracy and decay learning rate.\n","      if it % iterations_per_epoch == 0:\n","        # Check accuracy\n","        train_acc = (self.predict(X_batch) == y_batch).mean()\n","        val_acc = (self.predict(X_val) == y_val).mean()\n","        train_acc_history.append(train_acc)\n","        val_acc_history.append(val_acc)\n","\n","        # Decay learning rate\n","        learning_rate *= learning_rate_decay\n","\n","    return {\n","      'loss_history': loss_history,\n","      'train_acc_history': train_acc_history,\n","      'val_acc_history': val_acc_history,\n","    }\n","\n","  def predict(self, X):\n","    \"\"\"\n","    Use the trained weights of this two-layer network to predict labels for\n","    data points. For each data point we predict scores for each of the C\n","    classes, and assign each data point to the class with the highest score.\n","\n","    Inputs:\n","    - X: A numpy array of shape (N, D) giving N D-dimensional data points to\n","      classify.\n","\n","    Returns:\n","    - y_pred: A numpy array of shape (N,) giving predicted labels for each of\n","      the elements of X. For all i, y_pred[i] = c means that X[i] is predicted\n","      to have class c, where 0 <= c < C.\n","    \"\"\"\n","    y_pred = None\n","\n","    ###########################################################################\n","    # TODO: Implement this function; it should be VERY simple!                #\n","    ###########################################################################\n","    a1 = np.maximum( X.dot(self.params['W1']) + self.params['b1'], 0 )\n","    a2 = a1.dot(self.params['W2']) + self.params['b2']\n","    exp_a2 = pow(math.e, a2)\n","    scores   = (exp_a2.T/np.sum(exp_a2, axis = 1)).T\n","    \n","    y_pred = np.argmax(scores, axis = 1)\n","    ###########################################################################\n","    #                              END OF YOUR CODE                           #\n","    ###########################################################################\n","\n","    return y_pred\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"GIyS-blhJJkF","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["from __future__ import print_function\n","from past.builtins import xrange\n","\n","\n","from random import randrange\n","\n","def eval_numerical_gradient(f, x, verbose=True, h=0.00001):\n","  \"\"\" \n","  a naive implementation of numerical gradient of f at x \n","  - f should be a function that takes a single argument\n","  - x is the point (numpy array) to evaluate the gradient at\n","  \"\"\" \n","\n","  fx = f(x) # evaluate function value at original point\n","  grad = np.zeros_like(x)\n","  # iterate over all indexes in x\n","  it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n","  while not it.finished:\n","\n","    # evaluate function at x+h\n","    ix = it.multi_index\n","    oldval = x[ix]\n","    x[ix] = oldval + h # increment by h\n","    fxph = f(x) # evalute f(x + h)\n","    x[ix] = oldval - h\n","    fxmh = f(x) # evaluate f(x - h)\n","    x[ix] = oldval # restore\n","\n","    # compute the partial derivative with centered formula\n","    grad[ix] = (fxph - fxmh) / (2 * h) # the slope\n","    if verbose:\n","      print(ix, grad[ix])\n","    it.iternext() # step to next dimension\n","\n","  return grad\n","\n","\n","def eval_numerical_gradient_array(f, x, df, h=1e-5):\n","  \"\"\"\n","  Evaluate a numeric gradient for a function that accepts a numpy\n","  array and returns a numpy array.\n","  \"\"\"\n","  grad = np.zeros_like(x)\n","  it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n","  while not it.finished:\n","    ix = it.multi_index\n","    \n","    oldval = x[ix]\n","    x[ix] = oldval + h\n","    pos = f(x).copy()\n","    x[ix] = oldval - h\n","    neg = f(x).copy()\n","    x[ix] = oldval\n","    \n","    grad[ix] = np.sum((pos - neg) * df) / (2 * h)\n","    it.iternext()\n","  return grad\n","\n","\n","def eval_numerical_gradient_blobs(f, inputs, output, h=1e-5):\n","  \"\"\"\n","  Compute numeric gradients for a function that operates on input\n","  and output blobs.\n","  \n","  We assume that f accepts several input blobs as arguments, followed by a blob\n","  into which outputs will be written. For example, f might be called like this:\n","\n","  f(x, w, out)\n","  \n","  where x and w are input Blobs, and the result of f will be written to out.\n","\n","  Inputs: \n","  - f: function\n","  - inputs: tuple of input blobs\n","  - output: output blob\n","  - h: step size\n","  \"\"\"\n","  numeric_diffs = []\n","  for input_blob in inputs:\n","    diff = np.zeros_like(input_blob.diffs)\n","    it = np.nditer(input_blob.vals, flags=['multi_index'],\n","                   op_flags=['readwrite'])\n","    while not it.finished:\n","      idx = it.multi_index\n","      orig = input_blob.vals[idx]\n","\n","      input_blob.vals[idx] = orig + h\n","      f(*(inputs + (output,)))\n","      pos = np.copy(output.vals)\n","      input_blob.vals[idx] = orig - h\n","      f(*(inputs + (output,)))\n","      neg = np.copy(output.vals)\n","      input_blob.vals[idx] = orig\n","      \n","      diff[idx] = np.sum((pos - neg) * output.diffs) / (2.0 * h)\n","\n","      it.iternext()\n","    numeric_diffs.append(diff)\n","  return numeric_diffs\n","\n","\n","def eval_numerical_gradient_net(net, inputs, output, h=1e-5):\n","  return eval_numerical_gradient_blobs(lambda *args: net.forward(),\n","              inputs, output, h=h)\n","\n","\n","def grad_check_sparse(f, x, analytic_grad, num_checks=10, h=1e-5):\n","  \"\"\"\n","  sample a few random elements and only return numerical\n","  in this dimensions.\n","  \"\"\"\n","\n","  for i in xrange(num_checks):\n","    ix = tuple([randrange(m) for m in x.shape])\n","\n","    oldval = x[ix]\n","    x[ix] = oldval + h # increment by h\n","    fxph = f(x) # evaluate f(x + h)\n","    x[ix] = oldval - h # increment by h\n","    fxmh = f(x) # evaluate f(x - h)\n","    x[ix] = oldval # reset\n","\n","    grad_numerical = (fxph - fxmh) / (2 * h)\n","    grad_analytic = analytic_grad[ix]\n","    rel_error = abs(grad_numerical - grad_analytic) / (abs(grad_numerical) + abs(grad_analytic))\n","    print('numerical: %f analytic: %f, relative error: %e' % (grad_numerical, grad_analytic, rel_error))\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vQ89-PU1NR30","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["from __future__ import print_function\n","\n","from six.moves import cPickle as pickle\n","import numpy as np\n","import os\n","from imageio import  imread\n","import platform\n","\n","def load_pickle(f):\n","    version = platform.python_version_tuple()\n","    if version[0] == '2':\n","        return  pickle.load(f)\n","    elif version[0] == '3':\n","        return  pickle.load(f, encoding='latin1')\n","    raise ValueError(\"invalid python version: {}\".format(version))\n","\n","def load_CIFAR_batch(filename):\n","  \"\"\" load single batch of cifar \"\"\"\n","  with open(filename, 'rb') as f:\n","    datadict = load_pickle(f)\n","    X = datadict['data']\n","    Y = datadict['labels']\n","    X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"float\")\n","    Y = np.array(Y)\n","    return X, Y\n","\n","def load_CIFAR10(ROOT):\n","  \"\"\" load all of cifar \"\"\"\n","  xs = []\n","  ys = []\n","  for b in range(1,6):\n","    f = os.path.join(ROOT, 'data_batch_%d' % (b, ))\n","    X, Y = load_CIFAR_batch(f)\n","    xs.append(X)\n","    ys.append(Y)    \n","  Xtr = np.concatenate(xs)\n","  Ytr = np.concatenate(ys)\n","  del X, Y\n","  Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, 'test_batch'))\n","  return Xtr, Ytr, Xte, Yte\n","\n","\n","def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000,\n","                     subtract_mean=True):\n","    \"\"\"\n","    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n","    it for classifiers. These are the same steps as we used for the SVM, but\n","    condensed to a single function.\n","    \"\"\"\n","    # Load the raw CIFAR-10 data\n","    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n","    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n","        \n","    # Subsample the data\n","    mask = list(range(num_training, num_training + num_validation))\n","    X_val = X_train[mask]\n","    y_val = y_train[mask]\n","    mask = list(range(num_training))\n","    X_train = X_train[mask]\n","    y_train = y_train[mask]\n","    mask = list(range(num_test))\n","    X_test = X_test[mask]\n","    y_test = y_test[mask]\n","\n","    # Normalize the data: subtract the mean image\n","    if subtract_mean:\n","      mean_image = np.mean(X_train, axis=0)\n","      X_train -= mean_image\n","      X_val -= mean_image\n","      X_test -= mean_image\n","    \n","    # Transpose so that channels come first\n","    X_train = X_train.transpose(0, 3, 1, 2).copy()\n","    X_val = X_val.transpose(0, 3, 1, 2).copy()\n","    X_test = X_test.transpose(0, 3, 1, 2).copy()\n","\n","    # Package data into a dictionary\n","    return {\n","      'X_train': X_train, 'y_train': y_train,\n","      'X_val': X_val, 'y_val': y_val,\n","      'X_test': X_test, 'y_test': y_test,\n","    }\n","    \n","\n","def load_tiny_imagenet(path, dtype=np.float32, subtract_mean=True):\n","  \"\"\"\n","  Load TinyImageNet. Each of TinyImageNet-100-A, TinyImageNet-100-B, and\n","  TinyImageNet-200 have the same directory structure, so this can be used\n","  to load any of them.\n","\n","  Inputs:\n","  - path: String giving path to the directory to load.\n","  - dtype: numpy datatype used to load the data.\n","  - subtract_mean: Whether to subtract the mean training image.\n","\n","  Returns: A dictionary with the following entries:\n","  - class_names: A list where class_names[i] is a list of strings giving the\n","    WordNet names for class i in the loaded dataset.\n","  - X_train: (N_tr, 3, 64, 64) array of training images\n","  - y_train: (N_tr,) array of training labels\n","  - X_val: (N_val, 3, 64, 64) array of validation images\n","  - y_val: (N_val,) array of validation labels\n","  - X_test: (N_test, 3, 64, 64) array of testing images.\n","  - y_test: (N_test,) array of test labels; if test labels are not available\n","    (such as in student code) then y_test will be None.\n","  - mean_image: (3, 64, 64) array giving mean training image\n","  \"\"\"\n","  # First load wnids\n","  with open(os.path.join(path, 'wnids.txt'), 'r') as f:\n","    wnids = [x.strip() for x in f]\n","\n","  # Map wnids to integer labels\n","  wnid_to_label = {wnid: i for i, wnid in enumerate(wnids)}\n","\n","  # Use words.txt to get names for each class\n","  with open(os.path.join(path, 'words.txt'), 'r') as f:\n","    wnid_to_words = dict(line.split('\\t') for line in f)\n","    for wnid, words in wnid_to_words.iteritems():\n","      wnid_to_words[wnid] = [w.strip() for w in words.split(',')]\n","  class_names = [wnid_to_words[wnid] for wnid in wnids]\n","\n","  # Next load training data.\n","  X_train = []\n","  y_train = []\n","  for i, wnid in enumerate(wnids):\n","    if (i + 1) % 20 == 0:\n","      print('loading training data for synset %d / %d' % (i + 1, len(wnids)))\n","    # To figure out the filenames we need to open the boxes file\n","    boxes_file = os.path.join(path, 'train', wnid, '%s_boxes.txt' % wnid)\n","    with open(boxes_file, 'r') as f:\n","      filenames = [x.split('\\t')[0] for x in f]\n","    num_images = len(filenames)\n","    \n","    X_train_block = np.zeros((num_images, 3, 64, 64), dtype=dtype)\n","    y_train_block = wnid_to_label[wnid] * np.ones(num_images, dtype=np.int64)\n","    for j, img_file in enumerate(filenames):\n","      img_file = os.path.join(path, 'train', wnid, 'images', img_file)\n","      img = imread(img_file)\n","      if img.ndim == 2:\n","        ## grayscale file\n","        img.shape = (64, 64, 1)\n","      X_train_block[j] = img.transpose(2, 0, 1)\n","    X_train.append(X_train_block)\n","    y_train.append(y_train_block)\n","      \n","  # We need to concatenate all training data\n","  X_train = np.concatenate(X_train, axis=0)\n","  y_train = np.concatenate(y_train, axis=0)\n","  \n","  # Next load validation data\n","  with open(os.path.join(path, 'val', 'val_annotations.txt'), 'r') as f:\n","    img_files = []\n","    val_wnids = []\n","    for line in f:\n","      img_file, wnid = line.split('\\t')[:2]\n","      img_files.append(img_file)\n","      val_wnids.append(wnid)\n","    num_val = len(img_files)\n","    y_val = np.array([wnid_to_label[wnid] for wnid in val_wnids])\n","    X_val = np.zeros((num_val, 3, 64, 64), dtype=dtype)\n","    for i, img_file in enumerate(img_files):\n","      img_file = os.path.join(path, 'val', 'images', img_file)\n","      img = imread(img_file)\n","      if img.ndim == 2:\n","        img.shape = (64, 64, 1)\n","      X_val[i] = img.transpose(2, 0, 1)\n","\n","  # Next load test images\n","  # Students won't have test labels, so we need to iterate over files in the\n","  # images directory.\n","  img_files = os.listdir(os.path.join(path, 'test', 'images'))\n","  X_test = np.zeros((len(img_files), 3, 64, 64), dtype=dtype)\n","  for i, img_file in enumerate(img_files):\n","    img_file = os.path.join(path, 'test', 'images', img_file)\n","    img = imread(img_file)\n","    if img.ndim == 2:\n","      img.shape = (64, 64, 1)\n","    X_test[i] = img.transpose(2, 0, 1)\n","\n","  y_test = None\n","  y_test_file = os.path.join(path, 'test', 'test_annotations.txt')\n","  if os.path.isfile(y_test_file):\n","    with open(y_test_file, 'r') as f:\n","      img_file_to_wnid = {}\n","      for line in f:\n","        line = line.split('\\t')\n","        img_file_to_wnid[line[0]] = line[1]\n","    y_test = [wnid_to_label[img_file_to_wnid[img_file]] for img_file in img_files]\n","    y_test = np.array(y_test)\n","  \n","  mean_image = X_train.mean(axis=0)\n","  if subtract_mean:\n","    X_train -= mean_image[None]\n","    X_val -= mean_image[None]\n","    X_test -= mean_image[None]\n","\n","  return {\n","    'class_names': class_names,\n","    'X_train': X_train,\n","    'y_train': y_train,\n","    'X_val': X_val,\n","    'y_val': y_val,\n","    'X_test': X_test,\n","    'y_test': y_test,\n","    'class_names': class_names,\n","    'mean_image': mean_image,\n","  }\n","\n","\n","def load_models(models_dir):\n","  \"\"\"\n","  Load saved models from disk. This will attempt to unpickle all files in a\n","  directory; any files that give errors on unpickling (such as README.txt) will\n","  be skipped.\n","\n","  Inputs:\n","  - models_dir: String giving the path to a directory containing model files.\n","    Each model file is a pickled dictionary with a 'model' field.\n","\n","  Returns:\n","  A dictionary mapping model file names to models.\n","  \"\"\"\n","  models = {}\n","  for model_file in os.listdir(models_dir):\n","    with open(os.path.join(models_dir, model_file), 'rb') as f:\n","      try:\n","        models[model_file] = load_pickle(f)['model']\n","      except pickle.UnpicklingError:\n","        continue\n","  return models\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9QmsJ9f-nJRN","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1}],"base_uri":"https://localhost:8080/","height":51},"outputId":"d6673c58-b448-4d25-fe73-4f274f1606a3","executionInfo":{"status":"ok","timestamp":1521125384753,"user_tz":-360,"elapsed":910,"user":{"displayName":"cat vinchy","photoUrl":"//lh4.googleusercontent.com/-GsapDbBWlnA/AAAAAAAAAAI/AAAAAAAAAVg/O2pmqJ62kG4/s50-c-k-no/photo.jpg","userId":"114142132153676042384"}}},"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","\n","\n","from __future__ import print_function\n","\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'\n","\n","# for auto-reloading external modules\n","# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n","%load_ext autoreload\n","%autoreload 2\n","\n","def rel_error(x, y):\n","    \"\"\" returns relative error \"\"\"\n","    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"],"execution_count":24,"outputs":[{"output_type":"stream","text":["The autoreload extension is already loaded. To reload it, use:\n","  %reload_ext autoreload\n"],"name":"stdout"}]},{"metadata":{"id":"AdFlrqN9qR-1","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["input_size = 4\n","hidden_size = 10\n","num_classes = 3\n","num_inputs = 5\n","\n","def init_toy_model():\n","    np.random.seed(0)\n","    return TwoLayerNet(input_size, hidden_size, num_classes, std=1e-1)\n","\n","def init_toy_data():\n","    np.random.seed(1)\n","    X = 10 * np.random.randn(num_inputs, input_size)\n","    y = np.array([0, 1, 2, 2, 1])\n","    return X, y\n","\n","net = init_toy_model()\n","X, y = init_toy_data()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"MLIMz0CoxabS","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1}],"base_uri":"https://localhost:8080/","height":118},"outputId":"efb10a9c-3006-4e50-d7d7-06a7e447ccac","executionInfo":{"status":"ok","timestamp":1521125388147,"user_tz":-360,"elapsed":871,"user":{"displayName":"cat vinchy","photoUrl":"//lh4.googleusercontent.com/-GsapDbBWlnA/AAAAAAAAAAI/AAAAAAAAAVg/O2pmqJ62kG4/s50-c-k-no/photo.jpg","userId":"114142132153676042384"}}},"cell_type":"code","source":["print(X)\n","print(y)"],"execution_count":26,"outputs":[{"output_type":"stream","text":["[[ 16.24345364  -6.11756414  -5.28171752 -10.72968622]\n"," [  8.65407629 -23.01538697  17.44811764  -7.61206901]\n"," [  3.19039096  -2.49370375  14.62107937 -20.60140709]\n"," [ -3.22417204  -3.84054355  11.33769442 -10.99891267]\n"," [ -1.72428208  -8.77858418   0.42213747   5.82815214]]\n","[0 1 2 2 1]\n"],"name":"stdout"}]},{"metadata":{"id":"e_HSqcYGPffC","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1}],"base_uri":"https://localhost:8080/","height":286},"outputId":"4f2aa315-0dce-4745-e99f-812d507705af","executionInfo":{"status":"ok","timestamp":1521125389866,"user_tz":-360,"elapsed":930,"user":{"displayName":"cat vinchy","photoUrl":"//lh4.googleusercontent.com/-GsapDbBWlnA/AAAAAAAAAAI/AAAAAAAAAVg/O2pmqJ62kG4/s50-c-k-no/photo.jpg","userId":"114142132153676042384"}}},"cell_type":"code","source":["scores = net.loss(X)\n","print('Your scores:')\n","print(scores)\n","print()\n","print('correct scores:')\n","correct_scores = np.asarray([\n","  [-0.81233741, -1.27654624, -0.70335995],\n","  [-0.17129677, -1.18803311, -0.47310444],\n","  [-0.51590475, -1.01354314, -0.8504215 ],\n","  [-0.15419291, -0.48629638, -0.52901952],\n","  [-0.00618733, -0.12435261, -0.15226949]])\n","print(correct_scores)\n","print()\n","\n","# The difference should be very small. We get < 1e-7\n","print('Difference between your scores and correct scores:')\n","print(np.sum(np.abs(scores - correct_scores)))"],"execution_count":27,"outputs":[{"output_type":"stream","text":["Your scores:\n","[[-0.81233741 -1.27654624 -0.70335995]\n"," [-0.17129677 -1.18803311 -0.47310444]\n"," [-0.51590475 -1.01354314 -0.8504215 ]\n"," [-0.15419291 -0.48629638 -0.52901952]\n"," [-0.00618733 -0.12435261 -0.15226949]]\n","\n","correct scores:\n","[[-0.81233741 -1.27654624 -0.70335995]\n"," [-0.17129677 -1.18803311 -0.47310444]\n"," [-0.51590475 -1.01354314 -0.8504215 ]\n"," [-0.15419291 -0.48629638 -0.52901952]\n"," [-0.00618733 -0.12435261 -0.15226949]]\n","\n","Difference between your scores and correct scores:\n","3.6802720745909845e-08\n"],"name":"stdout"}]},{"metadata":{"id":"Kc-cp1C7xIFF","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1}],"base_uri":"https://localhost:8080/","height":51},"outputId":"049a4880-2a5d-42a4-f112-391c5b39ec93","executionInfo":{"status":"ok","timestamp":1521125392015,"user_tz":-360,"elapsed":1087,"user":{"displayName":"cat vinchy","photoUrl":"//lh4.googleusercontent.com/-GsapDbBWlnA/AAAAAAAAAAI/AAAAAAAAAVg/O2pmqJ62kG4/s50-c-k-no/photo.jpg","userId":"114142132153676042384"}}},"cell_type":"code","source":["loss, _ = net.loss(X, y, reg=0.05)\n","correct_loss = 1.30378789133\n","\n","# should be very small, we get < 1e-12\n","print('Difference between your loss and correct loss:')\n","print(np.sum(np.abs(loss - correct_loss)))"],"execution_count":28,"outputs":[{"output_type":"stream","text":["Difference between your loss and correct loss:\n","0.01896541960606335\n"],"name":"stdout"}]},{"metadata":{"id":"iimrnROhGpJf","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1}],"base_uri":"https://localhost:8080/","height":84},"outputId":"f9cacdd8-0d98-45c6-9a41-da2c88a62559","executionInfo":{"status":"ok","timestamp":1521125508346,"user_tz":-360,"elapsed":911,"user":{"displayName":"cat vinchy","photoUrl":"//lh4.googleusercontent.com/-GsapDbBWlnA/AAAAAAAAAAI/AAAAAAAAAVg/O2pmqJ62kG4/s50-c-k-no/photo.jpg","userId":"114142132153676042384"}}},"cell_type":"code","source":["loss, grads = net.loss(X, y, reg=0.05)\n","\n","# these should all be less than 1e-8 or so\n","for param_name in grads:\n","    f = lambda W: net.loss(X, y, reg=0.05)[0]\n","    param_grad_num = eval_numerical_gradient(f, net.params[param_name], verbose=False)\n","    print('%s max relative error: %e' % (param_name, rel_error(param_grad_num, grads[param_name])))"],"execution_count":30,"outputs":[{"output_type":"stream","text":["W1 max relative error: 4.090897e-09\n","W2 max relative error: 3.440708e-09\n","b2 max relative error: 4.447646e-11\n","b1 max relative error: 2.738420e-09\n"],"name":"stdout"}]},{"metadata":{"id":"0RrAYKXyJUEd","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1},{"item_id":2}],"base_uri":"https://localhost:8080/","height":529},"outputId":"f9d29194-07ba-4592-ebdd-e3fc21f85eb2","executionInfo":{"status":"ok","timestamp":1521126134030,"user_tz":-360,"elapsed":1143,"user":{"displayName":"cat vinchy","photoUrl":"//lh4.googleusercontent.com/-GsapDbBWlnA/AAAAAAAAAAI/AAAAAAAAAVg/O2pmqJ62kG4/s50-c-k-no/photo.jpg","userId":"114142132153676042384"}}},"cell_type":"code","source":["\n","net = init_toy_model()\n","stats = net.train(X, y, X, y,\n","            learning_rate=1e-1, reg=1e-5,\n","            num_iters=100, batch_size= 5, verbose=False)\n","\n","print ('Final training loss: ', stats['loss_history'][-1])\n","\n","# plot the loss history\n","plt.plot(stats['loss_history'])\n","plt.xlabel('iteration')\n","plt.ylabel('training loss')\n","plt.title('Training Loss history')\n","plt.show()"],"execution_count":33,"outputs":[{"output_type":"stream","text":["Final training loss:  0.01563498761185673\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAl8AAAHvCAYAAABqnbr1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<matplotlib.figure.Figure at 0x7f9aee684610>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"XVHbecwTJfAs","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1}],"base_uri":"https://localhost:8080/","height":582},"outputId":"93213fd3-59eb-4b04-bc23-d3cf3910ca02","executionInfo":{"status":"error","timestamp":1521127052174,"user_tz":-360,"elapsed":935,"user":{"displayName":"cat vinchy","photoUrl":"//lh4.googleusercontent.com/-GsapDbBWlnA/AAAAAAAAAAI/AAAAAAAAAVg/O2pmqJ62kG4/s50-c-k-no/photo.jpg","userId":"114142132153676042384"}}},"cell_type":"code","source":["\n","def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000):\n","    \"\"\"\n","    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n","    it for the two-layer neural net classifier. These are the same steps as\n","    we used for the SVM, but condensed to a single function.  \n","    \"\"\"\n","    # Load the raw CIFAR-10 data\n","    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n","    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n","        \n","    # Subsample the data\n","    mask = list(range(num_training, num_training + num_validation))\n","    X_val = X_train[mask]\n","    y_val = y_train[mask]\n","    mask = list(range(num_training))\n","    X_train = X_train[mask]\n","    y_train = y_train[mask]\n","    mask = list(range(num_test))\n","    X_test = X_test[mask]\n","    y_test = y_test[mask]\n","\n","    # Normalize the data: subtract the mean image\n","    mean_image = np.mean(X_train, axis=0)\n","    X_train -= mean_image\n","    X_val -= mean_image\n","    X_test -= mean_image\n","\n","    # Reshape data to rows\n","    X_train = X_train.reshape(num_training, -1)\n","    X_val = X_val.reshape(num_validation, -1)\n","    X_test = X_test.reshape(num_test, -1)\n","\n","    return X_train, y_train, X_val, y_val, X_test, y_test\n","\n","\n","# Invoke the above function to get our data.\n","X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()\n","print('Train data shape: ', X_train.shape)\n","print('Train labels shape: ', y_train.shape)\n","print('Validation data shape: ', X_val.shape)\n","print('Validation labels shape: ', y_val.shape)\n","print('Test data shape: ', X_test.shape)\n","print('Test labels shape: ', y_test.shape)"],"execution_count":37,"outputs":[{"output_type":"error","ename":"IOError","evalue":"ignored","traceback":["\u001b[0;31m\u001b[0m","\u001b[0;31mIOError\u001b[0mTraceback (most recent call last)","\u001b[0;32m<ipython-input-37-0ab2bc22b843>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# Invoke the above function to get our data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_CIFAR10_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train data shape: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train labels shape: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-37-0ab2bc22b843>\u001b[0m in \u001b[0;36mget_CIFAR10_data\u001b[0;34m(num_training, num_validation, num_test)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Load the raw CIFAR-10 data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mcifar10_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cs231n/datasets/cifar-10-batches-py'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_CIFAR10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcifar10_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Subsample the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-35-1c332d6f31b6>\u001b[0m in \u001b[0;36mload_CIFAR10\u001b[0;34m(ROOT)\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mROOT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data_batch_%d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_CIFAR_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mxs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-35-1c332d6f31b6>\u001b[0m in \u001b[0;36mload_CIFAR_batch\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_CIFAR_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0;34m\"\"\" load single batch of cifar \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m   \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mdatadict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatadict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'cs231n/datasets/cifar-10-batches-py/data_batch_1'"]}]},{"metadata":{"id":"KdkOTwPjPKlL","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}